{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bbacec2",
   "metadata": {},
   "source": [
    "# Markov Chain Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e330c9",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo (MCMC): Sampling the Impossible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6085d",
   "metadata": {},
   "source": [
    "## 1. The Core Problem MCMC Solves\n",
    "\n",
    "In Bayesian inference, we often want the posterior distribution:\n",
    "\n",
    "\\[\n",
    "p(\\theta \\mid x) = \\frac{p(x \\mid \\theta)\\, p(\\theta)}{p(x)}\n",
    "\\]\n",
    "\n",
    "The problem is the denominator:\n",
    "\n",
    "\\[\n",
    "p(x) = \\int p(x \\mid \\theta)\\, p(\\theta)\\, d\\theta\n",
    "\\]\n",
    "\n",
    "For anything non-trivial, this integral is:\n",
    "\n",
    "- High-dimensional  \n",
    "- Intractable  \n",
    "- Impossible to compute analytically  \n",
    "\n",
    "**MCMC exists because exact Bayesian inference usually cannot be done.**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What Is Markov Chain Monte Carlo?\n",
    "\n",
    "**Markov Chain Monte Carlo (MCMC)** is a family of algorithms that:\n",
    "\n",
    "> Generate samples from a target probability distribution by constructing a Markov chain whose stationary distribution is that target distribution.\n",
    "\n",
    "### Key idea\n",
    "\n",
    "- You don’t compute the distribution  \n",
    "- You sample from it  \n",
    "- Expectations become sample averages  \n",
    "\n",
    "Formally, if:\n",
    "\n",
    "\\[\n",
    "\\theta^{(1)}, \\ldots, \\theta^{(N)} \\sim p(\\theta \\mid x)\n",
    "\\]\n",
    "\n",
    "Then expectations can be approximated as:\n",
    "\n",
    "\\[\n",
    "\\mathbb{E}[f(\\theta)] \\approx \\frac{1}{N} \\sum_{i=1}^{N} f(\\theta^{(i)})\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why “Markov Chain”?\n",
    "\n",
    "A Markov chain satisfies:\n",
    "\n",
    "\\[\n",
    "p(\\theta^{(t+1)} \\mid \\theta^{(t)}, \\ldots, \\theta^{(1)})\n",
    "=\n",
    "p(\\theta^{(t+1)} \\mid \\theta^{(t)})\n",
    "\\]\n",
    "\n",
    "Meaning:\n",
    "\n",
    "- The next sample depends only on the current one  \n",
    "- Transitions are **memoryless**  \n",
    "\n",
    "This constraint makes sampling feasible—but introduces **correlation** between samples.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Why “Monte Carlo”?\n",
    "\n",
    "“Monte Carlo” refers to:\n",
    "\n",
    "- Using randomness to approximate expectations  \n",
    "- Replacing integrals with averages  \n",
    "\n",
    "**MCMC = Monte Carlo integration using dependent samples generated by a Markov chain**\n",
    "\n",
    "---\n",
    "\n",
    "## 5. The General MCMC Workflow\n",
    "\n",
    "Every MCMC method follows this structure:\n",
    "\n",
    "1. Define target density \\( p(\\theta) \\) (up to a constant)  \n",
    "2. Initialize \\( \\theta^{(0)} \\)  \n",
    "3. Propose a new state \\( \\theta' \\)  \n",
    "4. Decide whether to accept it  \n",
    "5. Repeat until convergence  \n",
    "\n",
    "The key trick is step 4:  \n",
    "**accept/reject without knowing the normalizing constant**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Metropolis–Hastings: The Canonical Algorithm\n",
    "\n",
    "### Step 1: Proposal Distribution\n",
    "\n",
    "Propose a new state:\n",
    "\n",
    "\\[\n",
    "\\theta' \\sim q(\\theta' \\mid \\theta^{(t)})\n",
    "\\]\n",
    "\n",
    "Common choice:  \n",
    "- Gaussian random walk\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Acceptance Probability\n",
    "\n",
    "Compute:\n",
    "\n",
    "\\[\n",
    "\\alpha = \\min\\left(\n",
    "1,\n",
    "\\frac{p(\\theta')\\, q(\\theta^{(t)} \\mid \\theta')}\n",
    "     {p(\\theta^{(t)})\\, q(\\theta' \\mid \\theta^{(t)})}\n",
    "\\right)\n",
    "\\]\n",
    "\n",
    "**Key insight:**\n",
    "\n",
    "- The normalizing constant cancels out  \n",
    "- Only the **unnormalized density** is required  \n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Accept or Reject\n",
    "\n",
    "\\[\n",
    "\\theta^{(t+1)} =\n",
    "\\begin{cases}\n",
    "\\theta', & \\text{with probability } \\alpha \\\\\n",
    "\\theta^{(t)}, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "Rejected proposals cause repeated samples, increasing **autocorrelation**.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Why This Works (Stationarity Intuition)\n",
    "\n",
    "Metropolis–Hastings enforces **detailed balance**:\n",
    "\n",
    "\\[\n",
    "p(\\theta)\\, T(\\theta \\to \\theta') =\n",
    "p(\\theta')\\, T(\\theta' \\to \\theta)\n",
    "\\]\n",
    "\n",
    "This guarantees:\n",
    "\n",
    "- The target distribution is **stationary**  \n",
    "- Long-run samples come from the correct posterior  \n",
    "\n",
    "⚠️ Stationarity does **not** imply fast convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Burn-in, Mixing, and Autocorrelation\n",
    "\n",
    "### Burn-in\n",
    "\n",
    "Early samples depend heavily on initialization.\n",
    "\n",
    "Solution:\n",
    "\n",
    "- Discard the first \\( N_{\\text{burn}} \\) samples\n",
    "\n",
    "---\n",
    "\n",
    "### Mixing\n",
    "\n",
    "Mixing measures how fast the chain explores the distribution.\n",
    "\n",
    "Poor mixing leads to:\n",
    "\n",
    "- Strong autocorrelation  \n",
    "- Inefficient sampling  \n",
    "\n",
    "---\n",
    "\n",
    "### Effective Sample Size (ESS)\n",
    "\n",
    "\\[\n",
    "\\text{ESS} < N\n",
    "\\]\n",
    "\n",
    "Even 100,000 samples may correspond to only a few thousand independent ones.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Hamiltonian Monte Carlo (Why Modern MCMC Exists)\n",
    "\n",
    "Random-walk MCMC scales poorly in high dimensions.\n",
    "\n",
    "**Hamiltonian Monte Carlo (HMC)** improves this by:\n",
    "\n",
    "- Using gradients of \\( \\log p(\\theta) \\)  \n",
    "- Introducing auxiliary momentum variables  \n",
    "- Following physics-inspired trajectories  \n",
    "\n",
    "Effects:\n",
    "\n",
    "- Longer proposals  \n",
    "- Lower autocorrelation  \n",
    "- Better scaling with dimension  \n",
    "\n",
    "Trade-offs:\n",
    "\n",
    "- Requires differentiable log-density  \n",
    "- More complex to implement  \n",
    "\n",
    "This is why tools like **Stan** and **PyMC** use **HMC / NUTS** by default.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. What MCMC Gives You (and What It Doesn’t)\n",
    "\n",
    "### What You Get\n",
    "\n",
    "- ✔ Samples from the posterior  \n",
    "- ✔ Uncertainty quantification  \n",
    "- ✔ Parameter correlations  \n",
    "- ✔ Bayesian credible intervals  \n",
    "\n",
    "### What You Don’t Get\n",
    "\n",
    "- ✖ Exact distributions  \n",
    "- ✖ Guaranteed fast convergence  \n",
    "- ✖ Scalability to massive datasets  \n",
    "\n",
    "**MCMC trades exactness for generality.**\n",
    "\n",
    "---\n",
    "\n",
    "## 11. When Should You Use MCMC?\n",
    "\n",
    "### Use MCMC when:\n",
    "\n",
    "- Posterior is complex  \n",
    "- Uncertainty matters  \n",
    "- Dataset is moderate-sized  \n",
    "- Model is hierarchical or fully Bayesian  \n",
    "\n",
    "### Avoid MCMC when:\n",
    "\n",
    "- Dataset is huge  \n",
    "- Posterior has a closed form  \n",
    "- Latency constraints exist  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e6aa9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
