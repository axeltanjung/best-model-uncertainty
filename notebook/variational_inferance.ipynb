{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a29282",
   "metadata": {},
   "source": [
    "# Variational Inference (VI)\n",
    "\n",
    "Variational Inference (VI) is a family of optimization-based methods for **approximating complex probability distributions**, most commonly used in **Bayesian inference** when exact posterior computation is intractable.\n",
    "\n",
    "Instead of sampling from the posterior (as in MCMC), VI **turns inference into an optimization problem**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Core Problem in Bayesian Inference\n",
    "\n",
    "In Bayesian modeling, we want the posterior distribution:\n",
    "\n",
    "\\[\n",
    "p(\\theta \\mid x) = \\frac{p(x \\mid \\theta)\\, p(\\theta)}{p(x)}\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( \\theta \\) = latent variables / parameters  \n",
    "- \\( x \\) = observed data  \n",
    "- \\( p(x) = \\int p(x \\mid \\theta)p(\\theta)\\, d\\theta \\) (marginal likelihood)\n",
    "\n",
    "### The bottleneck\n",
    "- The denominator \\( p(x) \\) is often **intractable**\n",
    "- High-dimensional integrals make exact inference impossible\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Key Idea of Variational Inference\n",
    "\n",
    "Instead of computing the true posterior \\( p(\\theta \\mid x) \\), VI:\n",
    "\n",
    "1. Chooses a **simpler family of distributions** \\( q(\\theta) \\)\n",
    "2. Finds the member of this family **closest** to the true posterior\n",
    "\n",
    "Closeness is measured using **KL divergence**:\n",
    "\n",
    "\\[\n",
    "\\text{KL}(q(\\theta) \\parallel p(\\theta \\mid x))\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Optimization Objective: ELBO\n",
    "\n",
    "Directly minimizing KL divergence to the posterior is impossible because it depends on \\( p(x) \\).  \n",
    "VI reformulates the problem using the **Evidence Lower Bound (ELBO)**.\n",
    "\n",
    "### ELBO definition\n",
    "\n",
    "\\[\n",
    "\\log p(x) \\ge\n",
    "\\mathbb{E}_{q(\\theta)}[\\log p(x,\\theta)]\n",
    "-\n",
    "\\mathbb{E}_{q(\\theta)}[\\log q(\\theta)]\n",
    "\\]\n",
    "\n",
    "This is equivalent to:\n",
    "\n",
    "\\[\n",
    "\\text{ELBO} =\n",
    "\\mathbb{E}_{q(\\theta)}[\\log p(x \\mid \\theta)]\n",
    "-\n",
    "\\text{KL}(q(\\theta)\\,\\|\\,p(\\theta))\n",
    "\\]\n",
    "\n",
    "### Interpretation\n",
    "- **First term**: data fit (expected log-likelihood)\n",
    "- **Second term**: regularization toward the prior\n",
    "\n",
    "Maximizing ELBO ⇔ minimizing KL divergence.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Variational Family Choices\n",
    "\n",
    "The approximation quality depends heavily on the choice of \\( q(\\theta) \\).\n",
    "\n",
    "### Mean-Field Variational Inference\n",
    "Assumes full factorization:\n",
    "\n",
    "\\[\n",
    "q(\\theta) = \\prod_{i} q_i(\\theta_i)\n",
    "\\]\n",
    "\n",
    "**Pros**\n",
    "- Simple\n",
    "- Fast\n",
    "- Scales well\n",
    "\n",
    "**Cons**\n",
    "- Ignores posterior correlations\n",
    "- Underestimates uncertainty\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Coordinate Ascent Variational Inference (CAVI)\n",
    "\n",
    "When conjugacy exists, VI can be solved analytically using coordinate updates.\n",
    "\n",
    "For each latent variable \\( \\theta_i \\):\n",
    "\n",
    "\\[\n",
    "\\log q_i^*(\\theta_i)\n",
    "=\n",
    "\\mathbb{E}_{-i}[\\log p(x,\\theta)]\n",
    "+ \\text{const}\n",
    "\\]\n",
    "\n",
    "Iteratively update each factor until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Stochastic Variational Inference (SVI)\n",
    "\n",
    "For large datasets, full-batch VI is infeasible.\n",
    "\n",
    "### SVI approach\n",
    "- Use **mini-batches**\n",
    "- Optimize ELBO using **stochastic gradients**\n",
    "- Enables VI on massive datasets\n",
    "\n",
    "Key ingredients:\n",
    "- Reparameterization trick\n",
    "- Natural gradients\n",
    "- Learning rate schedules\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Reparameterization Trick\n",
    "\n",
    "Used to reduce gradient variance.\n",
    "\n",
    "Instead of sampling:\n",
    "\\[\n",
    "\\theta \\sim q_\\phi(\\theta)\n",
    "\\]\n",
    "\n",
    "Rewrite as:\n",
    "\\[\n",
    "\\theta = g(\\epsilon, \\phi), \\quad \\epsilon \\sim p(\\epsilon)\n",
    "\\]\n",
    "\n",
    "Example (Gaussian):\n",
    "\\[\n",
    "\\theta = \\mu + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1)\n",
    "\\]\n",
    "\n",
    "This allows backpropagation through stochastic nodes.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Variational Inference vs MCMC\n",
    "\n",
    "| Aspect | Variational Inference | MCMC |\n",
    "|-----|----------------------|------|\n",
    "| Speed | Very fast | Slow |\n",
    "| Scalability | Excellent | Poor |\n",
    "| Bias | Biased | Asymptotically exact |\n",
    "| Uncertainty | Underestimated | Accurate |\n",
    "| Parallelization | Easy | Difficult |\n",
    "\n",
    "**Rule of thumb**\n",
    "- Use **VI** when speed and scale matter\n",
    "- Use **MCMC** when accuracy and uncertainty are critical\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Common Failure Modes of VI\n",
    "\n",
    "1. **Posterior collapse** (over-regularization)\n",
    "2. **Variance underestimation**\n",
    "3. **Bad variational family choice**\n",
    "4. **Local optima**\n",
    "5. **Poor initialization**\n",
    "\n",
    "VI is not “wrong” — it is **biased by design**.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Practical Applications\n",
    "\n",
    "- Topic models (LDA)\n",
    "- Bayesian neural networks\n",
    "- Variational Autoencoders (VAE)\n",
    "- Probabilistic graphical models\n",
    "- Online Bayesian learning\n",
    "\n",
    "---\n",
    "\n",
    "## 11. When Should You Use VI?\n",
    "\n",
    "Use Variational Inference when:\n",
    "- Data is large\n",
    "- Latent space is high-dimensional\n",
    "- Real-time or near-real-time inference is required\n",
    "- Approximate uncertainty is acceptable\n",
    "\n",
    "Avoid VI when:\n",
    "- Posterior correlations are critical\n",
    "- Exact uncertainty quantification is required\n",
    "- Dataset is small (MCMC is feasible)\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Summary\n",
    "\n",
    "- Variational Inference reframes Bayesian inference as optimization\n",
    "- ELBO is the central objective\n",
    "- Speed and scalability come at the cost of bias\n",
    "- VI is a **tool**, not a replacement for exact inference\n",
    "\n",
    "Understanding its assumptions is more important than memorizing formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99057c82",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
